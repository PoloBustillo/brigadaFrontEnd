# ğŸªª Algoritmo OCR para Credencial INE

> **Archivo relevante:** `lib/ocr/ine-ocr-parser.ts`  
> **Componente consumidor:** `components/survey/ine-question.tsx`  
> **Fecha:** 2026-02-21

---

## 1. Â¿QuÃ© devuelve ML Kit Text Recognition?

`@react-native-ml-kit/text-recognition` expone una jerarquÃ­a de tres niveles con **coordenadas de posiciÃ³n** para cada nivel:

```
VisionText
  â””â”€â”€ blocks[]          â† pÃ¡rrafos / regiones semÃ¡nticas
        â”œâ”€â”€ frame        { x, y, width, height }  â† bounding box en px
        â”œâ”€â”€ text         (todo el bloque como string)
        â””â”€â”€ lines[]
              â”œâ”€â”€ frame  { x, y, width, height }
              â”œâ”€â”€ text
              â””â”€â”€ elements[]    â† palabras individuales
                    â”œâ”€â”€ frame   { x, y, width, height }
                    â””â”€â”€ text
```

Cada `block`, `line` y `element` tiene un campo `frame` con la posiciÃ³n exacta del texto en la imagen original.

### Ejemplo de salida real (credencial INE 2019)

```json
{
  "blocks": [
    {
      "text": "INSTITUTO NACIONAL ELECTORAL",
      "frame": { "x": 12, "y": 5, "width": 280, "height": 14 }
    },
    {
      "text": "APELLIDO PATERNO\nGARCIA",
      "frame": { "x": 12, "y": 55, "width": 160, "height": 30 }
    },
    {
      "text": "CURP\nGARC901205HMCRZN09",
      "frame": { "x": 165, "y": 55, "width": 180, "height": 30 }
    }
  ]
}
```

---

## 2. Dos enfoques posibles: coordenadas vs. texto

| Enfoque                       | CÃ³mo funciona                                                                                                                        | Ventajas                                                                    | Desventajas                                                                                                          |
| ----------------------------- | ------------------------------------------------------------------------------------------------------------------------------------ | --------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------- |
| **Coordenadas (spatial)**     | Dividir la imagen en zonas fijas (p. ej. "CURP siempre estÃ¡ en X 165â€“345, Y 50â€“90") y mapear bloques a esas zonas por su `frame.x/y` | Muy preciso si el layout es constante                                       | El layout varÃ­a entre modelos A/B/C/D; requiere calibraciÃ³n por modelo; falla si la foto estÃ¡ rotada o perspectivada |
| **Texto relativo (adoptado)** | Extraer `.text` de cada bloque, concatenar en orden de lectura, aplicar regex + heurÃ­stica de "lÃ­nea siguiente a etiqueta"           | Funciona independientemente de tamaÃ±o y modelo de INE; tolera rotaciÃ³n leve | Depende de que ML Kit preserve el orden de bloques (generalmente lo hace topâ†’bottom, leftâ†’right)                     |

### Â¿Por quÃ© elegimos el enfoque de texto?

1. **4 modelos emitidos** (IFE 2008, IFE 2013, INE 2015, INE 2019): cada uno tiene un layout diferente. Las coordenadas absolutas de "CURP" varÃ­an en decenas de pÃ­xeles entre modelos.

2. **FotografÃ­as inclinadas**: la cÃ¡mara del telÃ©fono rara vez estÃ¡ perfectamente paralela a la credencial. Una inclinaciÃ³n de 10Â° desplaza los bloques varios pÃ­xeles, haciendo inÃºtil cualquier zona fija.

3. **Escalado**: la imagen capturada puede ser 1080Ã—700px o 3024Ã—2016px dependiendo del dispositivo. Las coordenadas absolutas son inÃºtiles sin normalizar al tamaÃ±o real de la tarjeta.

4. **Las etiquetas son el ancla**: INE imprime "APELLIDO PATERNO", "CURP", "CLAVE DE ELECTOR" en texto â€”usar esas etiquetas como anclas textuales es mÃ¡s robusto que sus coordenadas.

---

## 3. CuÃ¡ndo SÃ conviene usar coordenadas (trabajo futuro)

Si la precisiÃ³n actual de los nombres (estrategia heurÃ­stica â‰ˆ 70%) no es suficiente, se puede adoptar un enfoque hÃ­brido:

```
1. Detectar el modelo INE (ya implementado en detectIneModelo())
2. Por modelo, definir regiones de interÃ©s (ROI) relativas al tamaÃ±o de la tarjeta:
     MODELO_D: nombres en bloque inferior-izquierdo (y > 35% altura, x < 55% ancho)
     MODELO_C: nombres en franja central-izquierda (y 30-60%, x < 60%)
3. Filtrar blocks cuyo frame.y / imageHeight estÃ© dentro del ROI
4. Sobre ese subconjunto aplicar los regex actuales
```

Esto requiere pasar `imageWidth` / `imageHeight` al parser, o calcular los ROIs en porcentajes relativos al frame del bloque mÃ¡s alto y mÃ¡s bajo detectados.

---

## 4. Arquitectura del algoritmo actual

```
Camera capture (JPEG)
        â”‚
        â–¼
ML Kit TextRecognizer.recognize(imagePath)
        â”‚  returns VisionText { blocks[] }
        â–¼
blocks.map(b => b.text).join('\n')   â† descartamos frame por ahora
        â”‚
        â–¼
normalizeOcrText()
  â”œâ”€â”€ toUpperCase()
  â”œâ”€â”€ strip watermark chars (â—† â— â–  Â»)
  â”œâ”€â”€ collapse whitespace per line
  â””â”€â”€ drop lines < 2 chars
        â”‚
        â–¼
parseIneOcrText(frontText, backText)
  â”œâ”€â”€ CURP:           regex loose â†’ fixCurpOcr() â†’ regex strict
  â”œâ”€â”€ Clave Elector:  regex loose â†’ fixClaveElectorOcr() â†’ regex strict
  â”œâ”€â”€ Fecha Nac.:     3 formatos (DD/MM/YYYY, DD MMM YYYY, DDMMYYYY)
  â”œâ”€â”€ Sexo:           regex SEXO: [HM] | derivar de CURP[10]
  â”œâ”€â”€ SecciÃ³n:        regex SECCIÃ“N: NNNN
  â”œâ”€â”€ Vigencia:       regex VIGENCIA: YYYY
  â”œâ”€â”€ Nombres:        cascada (etiquetas â†’ bloque pre-CURP â†’ iniciales CURP)
  â””â”€â”€ Domicilio:      multi-lÃ­nea post-"DOMICILIO" hasta label token
        â”‚
        â–¼
IneOcrResult { ...campos, confidence, fieldConfidence }
        â”‚
        â–¼
ine-question.tsx renderiza campos + badge de confianza por campo
```

---

## 5. CorrecciÃ³n de confusiÃ³n OCR (posiciÃ³n a posiciÃ³n)

El parser aplica correcciones **posicionales** â€”distintas al layout de la imagenâ€” sobre las cadenas candidatas. Esto se basa en que CURP y Clave de Elector tienen un esquema `LETRA/DÃGITO` fijo por posiciÃ³n:

### CURP (18 chars)

| Posiciones | Tipo esperado         | Correcciones                             |
| ---------- | --------------------- | ---------------------------------------- |
| 0-3        | Letra                 | `0â†’O`, `1â†’I`, `8â†’B`, `5â†’S`, `2â†’Z`, `6â†’G` |
| 4-9        | DÃ­gito (fecha YYMMDD) | `Oâ†’0`, `Iâ†’1`, `Lâ†’1`, `Bâ†’8`, `Sâ†’5`, `Zâ†’2` |
| 10         | Letra (H/M)           | mismas correcciones de letra             |
| 11-13      | Letra (estado)        | mismas correcciones de letra             |
| 14-17      | AlfanumÃ©rico          | sin correcciÃ³n (ambos son vÃ¡lidos)       |

### Clave de Elector (18 chars)

| Posiciones | Tipo esperado | Correcciones           |
| ---------- | ------------- | ---------------------- |
| 0-5        | Letra         | `0â†’O`, `1â†’I`, etc.     |
| 6-13       | DÃ­gito        | `Oâ†’0`, `Iâ†’1`, etc.     |
| 14         | Letra (H/M)   | correcciones de letra  |
| 15-17      | DÃ­gito        | correcciones de dÃ­gito |

---

## 6. Confianza por campo

La confianza no es la confianza de ML Kit (que aplica a nivel de elemento grÃ¡fico, no semÃ¡ntico). Se calcula por la **estrategia de extracciÃ³n** usada:

| Estrategia                                   | Confianza | CuÃ¡ndo aplica                          |
| -------------------------------------------- | --------- | -------------------------------------- |
| Strict regex + validaciÃ³n post-correcciÃ³n    | **1.0**   | CURP/Clave que pasan el regex estricto |
| HeurÃ­stica con etiqueta explÃ­cita            | **0.9**   | Nombre via "APELLIDO PATERNO" label    |
| HeurÃ­stica sin etiqueta (bloque pre-CURP)    | **0.7**   | Nombre via posiciÃ³n relativa al CURP   |
| Loose regex / regex sin validaciÃ³n estricta  | **0.75**  | CURP/Clave que no pasan regex estricto |
| Derivado de otro campo (ej. sexo desde CURP) | **0.85**  | Sexo inferido de CURP[10]              |
| Fallback (iniciales desde CURP)              | **0.3**   | Nombres solo como iniciales            |
| No encontrado                                | **0.0**   | Campo vacÃ­o                            |

La confianza global es: `sum(fieldConf[i] para campos con valor) / totalCampos`

---

## 7. Limitaciones conocidas

| LimitaciÃ³n                           | Impacto                           | MitigaciÃ³n posible                                                 |
| ------------------------------------ | --------------------------------- | ------------------------------------------------------------------ |
| ML Kit no garantiza orden de bloques | Estrategia de bloque puede fallar | Usar `frame.y` para ordenar manualmente                            |
| No se usan coordenadas de ML Kit     | Perdemos informaciÃ³n de layout    | Implementar ROI por modelo (ver Â§3)                                |
| Credenciales muy reflejadas          | Texto demasiado ruidoso           | Preprocesar imagen (contrast/threshold) con expo-image-manipulator |
| MRZ en reverso de algunos modelos    | Confunde regex de CURP            | Filtrar lÃ­neas con `<<<` o dÃ­gitos de check                        |
| Solo probado con texto sintÃ©tico     | Resultados reales pueden diferir  | Unit tests con strings OCR reales capturados                       |

---

## 8. Uso del formato de confianza ML Kit (trabajo futuro)

ML Kit tambiÃ©n expone `confidence` a nivel de `element` (palabra). PodrÃ­a usarse para:

```ts
// Filtrar palabras con baja confianza antes de concatenar
const highConfidenceText = visionText.blocks
  .flatMap((b) => b.lines)
  .flatMap((l) => l.elements)
  .filter((e) => (e.confidence ?? 1) > 0.7)
  .map((e) => e.text)
  .join(" ");
```

Esto no estÃ¡ implementado en la versiÃ³n actual porque `@react-native-ml-kit/text-recognition` no expone `confidence` directamente en su interface TypeScript (depende de la versiÃ³n nativa).

---

## 9. CÃ³mo mejorar la precisiÃ³n (roadmap)

| #   | Mejora                                                                                                                        | Estado          |
| --- | ----------------------------------------------------------------------------------------------------------------------------- | --------------- |
| 1   | **Ordenar bloques por `frame.y`** â€” ordenar `blocks` por `frame.y` ascendente para garantizar orden topâ†’bottom                | âœ… Implementado |
| 2   | **Filtrar lÃ­neas MRZ** (`<<<`) del reverso que confunden el regex de CURP                                                     | âœ… Implementado |
| 3   | **Exponer `modeloDetected`** en `IneOcrResult` y mostrar badge en la UI                                                       | âœ… Implementado |
| 4   | **Imagen mÃ¡s grande para OCR** â€” resize 1200 â†’ 1600px antes de ML Kit                                                         | âœ… Implementado |
| 5   | **Tests unitarios con strings OCR reales** â€” fixtures en `lib/ocr/__tests__/ine-ocr-parser.test.ts`                           | â¬œ Pendiente    |
| 6   | **ROI por modelo** â€” filtros espaciales (`frame.y / imageHeight`) por modelo para nombres/domicilio                           | â¬œ Pendiente    |
| 7   | **Preprocesamiento avanzado** â€” contraste/grayscale cuando `expo-image-manipulator` lo soporte                                | â¬œ Pendiente    |
| 8   | **Endpoint OCR en backend** â€” FastAPI `/ocr/ine` con Google Vision `document_text_detection`                                  | â¬œ Pendiente    |
| 9   | **Mejorar domicilio** â€” expandir de 4 a 6 lÃ­neas y ajustar `LABEL_TOKENS` para modelos con colonia/municipio/estado separados | â¬œ Pendiente    |
